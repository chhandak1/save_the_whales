{"cells":[{"cell_type":"markdown","metadata":{"id":"coI69ryDvLZC"},"source":["# **Whale Inference Module**\n","\n","\n","---\n","Welcome to our Whale Inference Module!\n","\n","Given a directory of aerial whale images, this module will detect each whale in each image and predict its lenght axis, width measurements, and polygon mask.\n","\n","NOTE: If any of the steps fail or the Google Colab Notebook times out, please delete the session on Google Colab by going to ``Runtime -> Disconnect and delete runtime`` and start again."]},{"cell_type":"markdown","metadata":{"id":"6v_cB_P_iPxb"},"source":["## **Preliminary Setup**\n","\n","\n","---\n","\n","\n","In this section you will install PyTorch, mount/link your Google Drive, specify a base Google Drive directory location, and load the depedencies to run the model."]},{"cell_type":"markdown","metadata":{"id":"6eYmc51JipBk"},"source":["### **Step 1** : Mount your Google Drive\n","\n","Do only ONE of the options below\n","\n","**Option 1:**\n","\n","> On the left side bar click on the folder icon to open the *Files* dialog. \\\n","Next, click on the *Mount Drive* icon on the top of this dialog (i.e. the 3rd icon that looks like a folder with the Google Drive logo) \\\n","You will see a popup in the lower left corner saying \"Mounting Google Drive...\". Once your Google Drive is mounted you should see all of the contents of your drive under this *Files* dialog\n","\n","\n","**Option 2:**\n","\n","> Or you can run the code cell below, by clicking on the *Play* button on the cell. \\\n","This will create and link that you have to navigate to, in which you will have to copy an authentication token and paste in the input field that appears under the code\n","cell. \\\n","If successful it will print out a succesful mounting of your drive message.   \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsH5W6AX-k-w","outputId":"53af4fc5-a99a-4518-a6a6-7cc3baa9cb9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/mnt\n"]}],"source":["#@title\n","import os, sys\n","from google.colab import drive\n","drive.mount('/content/mnt')\n","nb_path = '/content/notebooks'\n"]},{"cell_type":"markdown","metadata":{"id":"A4CftX5Pj_vD"},"source":["### **Step 2** : Specify Directory of Technical Components\n","\n","In the form cell below, type in or paste in the path to the specific Google Drive folder you uploaded our provided depedency package and model code.\n","\n","Initially, you should see a placeholder name for the variable ` base_drive_dir `, which is meant to visualize what your input should look like. Pay close attention to end with “/”.\n","\n","The underlying code for the form cell will automatically run if you insert something new in the field(s). If you do not change out the value in the field you should click the *Play* button on the cell.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slHZHVmLhYai","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","outputId":"92551d22-b0a2-4528-81e7-6a217fb48755"},"outputs":[{"output_type":"stream","name":"stdout","text":["Base folder does not exist. Please re-enter path.\n"]}],"source":["#@title Base Google Drive Directory { run: \"auto\" }\n","#@markdown Please specify Google Drive directory that contains the dependecy package and model code provided for the field `base_drive_dir`\n","import os\n","base_drive_dir = \"/content/mnt/MyDrive\" #@param {type:\"string\"}\n","\n","if not os.path.isdir(base_drive_dir):\n","    print(\"Base folder does not exist. Please re-enter path.\")"]},{"cell_type":"markdown","metadata":{"id":"sKzuwb64kv4J"},"source":["### **Step 3** : Loading Dependencies\n","\n","In this step, we load the dependencies by simply clicking the *Play* button on the code cell below. This step may take some time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0f1d0RzrnfBp","cellView":"form"},"outputs":[],"source":["#@title\n","os.chdir(base_drive_dir)\n","\n","if not os.path.exists(os.path.join(base_drive_dir, 'detectron2')):\n","  print(\"Installing Detectron2:\")\n","  !git clone https://github.com/facebookresearch/detectron2.git\n","  !python -m pip install -e detectron2\n","  print(\"Detectron2 Installed\")\n","else:\n","  print(\"Detectron2 is already present. Installing Detectron2:\")\n","  !python -m pip install -e detectron2\n","  print(\"Detectron2 Installed\")\n","\n","os.chdir(os.path.join(base_drive_dir,'detectron2'))\n","\n","# Common imports (already available in Colab)\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import glob\n","import math\n","from collections import defaultdict\n","import datetime\n","from shapely.geometry import LineString\n","from math import hypot\n","import warnings\n","from tqdm.notebook import tqdm\n","import copy\n","import random\n","random.seed(42)\n","\n","# Detectron2 imports (not available in Colab)\n","import detectron2\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","from detectron2.engine import DefaultTrainer\n"]},{"cell_type":"markdown","metadata":{"id":"2_TDYjgDpK-a"},"source":["## **Running Inference**\n","\n","\n","---\n","\n","In this section you will first specify the input image directory you want to perform inference on and the output directory of where you want to see your results on. And then you will run the model to get an output."]},{"cell_type":"markdown","metadata":{"id":"o59iPbgrp72S"},"source":["### **Step 1** : Specify Image Directory & Output Directory\n","\n","In the form cell below, type in or paste in the path to the specific Google Drive folder for your input folder and the output folder.\n","\n","The `input_dir` field is where you should insert the directory that contains your whale images.\n","The `output_dir` field is where you should insert the directory that you want your predictions for your images to be written in.\n","\n","Initially, you should see a placeholder name for both the variables `input_dir` and `output_dir`, which is meant to visualize what your input should look like. Pay close attention to end with “/”.\n","\n","The underlying code for the form cell will automatically run if you insert something new in the field(s). If you do not change out the value in the field you should click the *Play* button on the cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3eDasutL3E_","cellView":"form"},"outputs":[],"source":["#@title Input & Output Directories { run: \"auto\" }\n","#@markdown Please insert the image directory path for `input_dir` and the directory you want your outputs in for `output_dir`\n","\n","input_dir = \"/content/mnt/MyDrive/\" #@param {type:\"string\"}\n","output_dir = \"/content/mnt/MyDrive/\" #@param {type:\"string\"}\n","\n","if not os.path.isdir(input_dir):\n","    print(\"Input folder does not exist. Please re-enter path.\")\n","\n","# if not os.path.isdir(output_dir):\n","#     print(\"Output folder does not exist. Please re-enter path.\")\n"]},{"cell_type":"markdown","metadata":{"id":"YQWTpbXJriB5"},"source":["### **Step 2** : Configure Model\n","\n","In this step, we make initial configurations to run inference by simply clicking the *Play* buttons next to the code cells below. Make sure to click each button in order and before clicking the next one make sure that the code cell has stopped running (i.e. the loading wheel on the *Play* will have stopped).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cn7igCwaQwS-","cellView":"form"},"outputs":[],"source":["#@title\n","\n","## Defining function for timeouts\n","# import errno\n","# import os\n","# import signal\n","# import functools\n","\n","# class TimeoutError(Exception):\n","#     pass\n","\n","# def timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n","#     def decorator(func):\n","#         def _handle_timeout(signum, frame):\n","#             raise TimeoutError(error_message)\n","\n","#         @functools.wraps(func)\n","#         def wrapper(*args, **kwargs):\n","#             signal.signal(signal.SIGALRM, _handle_timeout)\n","#             signal.alarm(seconds)\n","#             try:\n","#                 result = func(*args, **kwargs)\n","#             finally:\n","#                 signal.alarm(0)\n","#             return result\n","\n","#         return wrapper\n","\n","#     return decorator\n","\n","\n","os.chdir(os.path.join(base_drive_dir, 'Detectron2_Models.zip (Unzipped Files)'))\n","# os.chdir(base_drive_dir + 'Detectron2 Package')\n","\n","print(\"* Configuring model...\")\n","\n","## Setting up the configs for the keypoint detection model\n","cfg1 = get_cfg()\n","cfg1.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n","cfg1.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 5\n","cfg1.OUTPUT_DIR = os.path.join(base_drive_dir, 'Detectron2_Models.zip (Unzipped Files)/july_2023_keypoint_model/keypoint_detection_final_lr_0.00225_batch_size_2_epochs_2000_sbatch')\n","\n","## Setting up the configs for the segmentation model\n","cfg2 = get_cfg()\n","cfg2.MODEL.MASK_ON = True\n","cfg2.MODEL.KEYPOINT_ON = True\n","cfg2.OUTPUT_DIR = os.path.join(base_drive_dir, 'Detectron2_Models.zip (Unzipped Files)/july_2023_segmentation_model/segmentation_masks_final_lr_0.00225_batch_size_2_epochs_2500_sbatch')\n","cfg2.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg2.SOLVER.IMS_PER_BATCH = 2\n","cfg2.SOLVER.BASE_LR = 0.00025\n","cfg2.SOLVER.MAX_ITER = 2500\n","cfg2.SOLVER.STEPS = []        # do not decay learning rate\n","cfg2.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n","cfg2.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (whale)\n","cfg2.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 5\n","\n","\n","## Loading up the model weights for the keypoint model\n","cfg1.MODEL.WEIGHTS = os.path.join(cfg1.OUTPUT_DIR, \"model_final.pth\")\n","cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n","predictor1 = DefaultPredictor(cfg1)\n","print(\"* Done configuring Predictor 1\")\n","\n","## Loading up the model weights for the segmentation model\n","cfg2.MODEL.WEIGHTS = os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")\n","cfg2.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n","predictor2 = DefaultPredictor(cfg2)\n","print(\"* Done configuring Predictor 2\")\n","\n","## Find intersection between the segmentation mask and the 5% width lines\n","## These points are the endpoints of the 5% width lines\n","# @timeout(150)\n","def findIntersections(mask,slope_calc_x1,slope_calc_y1,slope_calc_x2,slope_calc_y2,xvalues,yvalues):\n","    ## Find all points on the 5% width lines where\n","    ## [aX,aY] and [bX,bY] are the endpoints to calculate the slope\n","    ## Line is drawn through [pX,pY]\n","    def getAllPointsOnLine(aX, aY, bX, bY,pX,pY, length,mask):\n","        vX = bX-aX\n","        vY = bY-aY\n","\n","        if(vX == 0 or vY == 0):\n","            return 0, 0, 0, 0\n","        mag = math.sqrt(vX*vX + vY*vY)\n","        vX = vX / mag\n","        vY = vY / mag\n","        temp = vX\n","        vX = 0-vY\n","        vY = temp\n","        cX=[]\n","        cY=[]\n","        dX=[]\n","        dY=[]\n","        for i in range(length):\n","            cX_temp=int(pX + vX * i)\n","            cY_temp=int(pY + vY * i)\n","            if cX_temp<mask.shape[1] and cY_temp<mask.shape[0] and cX_temp>=0 and cY_temp>=0:\n","                cX.append(cX_temp)\n","                cY.append(cY_temp)\n","\n","            dX_temp=int(pX - vX * i)\n","            dY_temp=int(pY - vY * i)\n","            if dX_temp<mask.shape[1] and dY_temp<mask.shape[0] and dX_temp>=0 and dY_temp>=0:\n","                dX.append(dX_temp)\n","                dY.append(dY_temp)\n","\n","        return (cX), (cY), (dX), (dY)\n","\n","    cXAll=[]\n","    cYAll=[]\n","    dXAll=[]\n","    dYAll=[]\n","\n","    intersection=defaultdict(list)\n","    for i in range(1,len(xvalues)-1):\n","        cX,cY,dX,dY=getAllPointsOnLine(slope_calc_x1[i],slope_calc_y1[i],slope_calc_x2[i],slope_calc_y2[i],xvalues[i],yvalues[i],400,mask)\n","\n","        for j in range(1,len(cX)-1):\n","\n","            if int(mask[cY[j]][cX[j]])!=int(mask[cY[j+1]][cX[j+1]]):\n","                intersection[i*5].append([cX[j],cY[j]])\n","            if int(mask[dY[j]][dX[j]])!=int(mask[dY[j+1]][dX[j+1]]):\n","                intersection[i*5].append([dX[j],dY[j]])\n","        if (i)*5 not in intersection.keys():\n","            intersection[i*5]=[]\n","\n","    return intersection\n","\n","\n","\n","## Find coordinates of the perpendicular line through [pX,pY]\n","## [aX,aY] and [bX,bY] are points on either side of [pX,pY] on the axis\n","# @timeout(150)\n","def getPerpCoord(aX, aY, bX, bY,pX,pY, length):\n","    vX = bX-aX\n","    vY = bY-aY\n","\n","    if(vX == 0 or vY == 0):\n","        return 0, 0, 0, 0\n","    mag = math.sqrt(vX*vX + vY*vY)\n","    vX = vX / mag\n","    vY = vY / mag\n","    temp = vX\n","    vX = 0-vY\n","    vY = temp\n","    cX = pX + vX * length\n","    cY = pY + vY * length\n","    dX = pX - vX * length\n","    dY = pY - vY * length\n","    return int(cX), int(cY), int(dX), int(dY)   # end getPerpCoord()\n","\n","\n","\n","## Find all points on the 5% width lines where\n","## [aX,aY] and [bX,bY] are the endpoints to calculate the slope\n","## Line is drawn through [pX,pY]\n","# @timeout(150)\n","def getAllPointsOnLine(aX, aY, bX, bY,pX,pY, length):\n","    vX = bX-aX\n","    vY = bY-aY\n","\n","    if(vX == 0 or vY == 0):\n","        return 0, 0, 0, 0\n","    mag = math.sqrt(vX*vX + vY*vY)\n","    vX = vX / mag\n","    vY = vY / mag\n","    temp = vX\n","    vX = 0-vY\n","    vY = temp\n","    cX=[]\n","    cY=[]\n","    dX=[]\n","    dY=[]\n","    for i in range(length):\n","        cX.append(int(pX + vX * length))\n","        cY.append(int(pY + vY * length))\n","        dX.append(int(pX - vX * length))\n","        dY.append(int(pY - vY * length))\n","    return (cX), (cY), (dX), (dY)   # end getAllPointsOnLine()\n","\n","\n","## Function to calculate the slope of the axis at 5% intervals\n","# @timeout(150)\n","def calculateSlopes(im, intersection_points, z):\n","    xvalues = intersection_points[:, 0]\n","    yvalues = intersection_points[:, 1]\n","    slope_calc_x1=[]\n","    slope_calc_x2=[]\n","\n","    for i in range(len(xvalues)):\n","        slope_calc_x1.append(xvalues[i]-2)\n","        slope_calc_x2.append(xvalues[i]+2)\n","\n","    slope_calc_y1=np.polyval(z,slope_calc_x1)\n","    slope_calc_y2=np.polyval(z,slope_calc_x2)\n","\n","    slopes=[]\n","\n","    for i in range(len(slope_calc_x1)):\n","        slopes.append((slope_calc_y2[i]-slope_calc_y1[i])/(slope_calc_y2[i]-slope_calc_y1[i]))\n","\n","    return slopes,slope_calc_x1,slope_calc_y1,slope_calc_x2,slope_calc_y2,xvalues,yvalues   # end calculateSlopes()\n","\n","\n","## Draw perpendicular lines at every 5% of the axis\n","# @timeout(150)\n","def drawPerps(im,slopes,slope_calc_x1,slopes_calc_y1,slopes_calc_x2,slopes_calc_y2,xvalues,yvalues):\n","    perpLines=[]\n","    for i in range(len(slopes)):\n","        perpLines.append(getPerpCoord(slope_calc_x1[i],slope_calc_y1[i],slope_calc_x2[i],slope_calc_y2[i],xvalues[i],yvalues[i],400))\n","\n","    for i in range(len(perpLines)):\n","        if i==len(perpLines)-4:\n","            im = cv2.line(im, [perpLines[i][0],perpLines[i][1]], [perpLines[i][2],perpLines[i][3]], (0,0,255), 2)\n","        else:\n","            im = cv2.line(im, [perpLines[i][0],perpLines[i][1]], [perpLines[i][2],perpLines[i][3]], (0,255,255), 2)\n","\n","    return im   # end drawPerps()\n","\n","\n","## Function to print the Whale ID on the top right corner of the image\n","# @timeout(150)\n","def markWhaleID(im,instance_no,index,instances):\n","    font = cv2.FONT_HERSHEY_DUPLEX\n","    pred_boxes= outputs2[\"instances\"].pred_boxes.tensor.cpu().numpy()\n","    org=(int(pred_boxes[index][0])+5,int(pred_boxes[index][1])+100)\n","    fontScale = 2\n","    color = (0, 238, 238)\n","    font_thickness = 2\n","    im = cv2.putText(im, 'Whale '+str(instance_no+1), org, font, fontScale, color, font_thickness, cv2.LINE_AA)\n","\n","    start_point = (int(pred_boxes[index][0]),int(pred_boxes[index][1]))\n","    end_point = (int(pred_boxes[index][2]),int(pred_boxes[index][3]))\n","    rectangle_thickness = 4\n","    im = cv2.rectangle(im, start_point, end_point, color, rectangle_thickness)\n","\n","\n","    return im   # end markWhaleID()\n","\n","\n","## Function to calculate Euclidean distance\n","# @timeout(150)\n","def calculateWhaleLength(x1, y1, x2, y2):\n","    return np.sqrt((x2-x1)**2 + (y2-y1)**2)\n","\n","## nseg = Number of segments the polynomial curve needs to be divided into\n","## z = polynomial curve\n","## x = X-coordinate of all keypoints\n","## y = Y-coordinate of all keypoints\n","# @timeout(150)\n","def divide_curve_into_n_segments(nseg, z, x, y):\n","    total_length = arclength(f, z, min(x), max(x))\n","    parts = 1001\n","    # nseg-=2\n","    seg_length = total_length/parts\n","    start_x = min(x)\n","    start_y = y[np.argmin(x)]\n","    theta = np.linspace(0, 2*np.pi, 100)\n","\n","    r = seg_length\n","    x1 = r*np.cos(theta)\n","    x2 = r*np.sin(theta)\n","    x_along_whale_axis = [x for x in range(min(x), max(x))]\n","    whale_axis = np.polyval(z, x_along_whale_axis)\n","\n","    all_intersection_points = np.zeros((parts,2))\n","\n","    for i in range(parts):\n","        new_x1 = x1 + start_x\n","        new_x2 = x2 + start_y\n","\n","        first_line = LineString(np.column_stack((new_x1, new_x2)))\n","        second_line = LineString(np.column_stack((x_along_whale_axis, whale_axis)))\n","        intersection = first_line.intersection(second_line)\n","        # print(\"Intersection geom:\", intersection.geom_type)\n","        if intersection.geom_type == 'MultiPoint':\n","            # intersection_points = np.asarray(intersection)\n","\n","            # for point in intersection_points:\n","            #     if point[0] > start_x:\n","            #         point_of_intersection = point\n","\n","            for point in intersection.geoms:\n","                if point.x > start_x:\n","                    point_of_intersection = np.asarray([point.x, point.y])\n","        elif intersection.geom_type == 'Point':\n","            # point_of_intersection = np.asarray(intersection)\n","            point_of_intersection = np.asarray([intersection.x, intersection.y])\n","        start_x = point_of_intersection[0]\n","        start_y = point_of_intersection[1]\n","        # print(type(point_of_intersection))\n","        # point_of_intersection = [point_of_intersection.x, point_of_intersection.y]\n","        all_intersection_points[i,:] = point_of_intersection\n","\n","    intersection_values_at_seg = np.zeros((nseg+1,2))\n","\n","    j=0\n","    for i in range(0, parts, parts//nseg):\n","        intersection_values_at_seg[j] = all_intersection_points[i,:]\n","        j+=1\n","#     plt.plot(x_along_whale_axis, whale_axis)\n","#     plt.plot(intersection_values_at_seg[:,0],intersection_values_at_seg[:,1], markersize = 5, marker=\"o\")\n","#     plt.show()\n","    if x[0]<x[-1]:\n","        return intersection_values_at_seg\n","    else:\n","        return np.flip(intersection_values_at_seg, axis = 0)\n","\n","# @timeout(150)\n","def f(z,x):\n","    return np.polyval(z,x)\n","\n","# @timeout(150)\n","def arclength(f, z, a, b, tol=1e-6):\n","    \"\"\"Compute the arc length of function f(x) for a <= x <= b. Stop\n","    when two consecutive approximations are closer than the value of\n","    tol.\n","    \"\"\"\n","    nsteps = 1  # number of steps to compute\n","    oldlength = 1.0e20\n","    length = 1.0e10\n","    while abs(oldlength - length) >= tol:\n","        nsteps *= 2\n","        fx1 = f(z,a)\n","        xdel = (b - a) / nsteps  # space between x-values\n","        oldlength = length\n","        length = 0\n","        for i in range(1, nsteps + 1):\n","            fx0 = fx1  # previous function value\n","            fx1 = f(z, a + i * (b - a) / nsteps)  # new function value\n","            length += hypot(xdel, fx1 - fx0)  # length of small line segment\n","    return length\n","\n","# @timeout(150)\n","def find_mappings(keypoint_output, segmentation_output):\n","    number_of_keypoint_instances = len(keypoint_output[\"instances\"].pred_boxes)\n","    number_of_segmentation_instances = len(segmentation_output[\"instances\"].pred_boxes)\n","\n","    keypoint_pred_boxes = keypoint_output[\"instances\"].pred_boxes.tensor.cpu().numpy()\n","    segmentation_pred_boxes = segmentation_output[\"instances\"].pred_boxes.tensor.cpu().numpy()\n","    mapping = []\n","    already_mapped_keypoints = set()\n","    already_mapped_segmentation = set()\n","    diff_dict = {}\n","\n","    # for k in range(len(keypoint_output[\"instances\"].pred_boxes)):\n","    for k in range(number_of_keypoint_instances):\n","        diff_arr = np.zeros(number_of_segmentation_instances)\n","        # for i in range(len(segmentation_output[\"instances\"].pred_boxes)):\n","        for i in range(number_of_segmentation_instances):\n","            difference = 0\n","            for j in range(4):\n","                difference = difference+abs(segmentation_pred_boxes[i, j]-keypoint_pred_boxes[k, j])\n","            diff_arr[i] = (difference)\n","            diff_dict[difference] = [k, i] #[Keypoint, Segmentation]\n","\n","    all_differences = list(diff_dict.keys())\n","    all_differences.sort()\n","\n","    number_of_instances_required = min(number_of_segmentation_instances, number_of_keypoint_instances)\n","    final_mapping = []\n","\n","    i = 0\n","\n","    while len(final_mapping)!= number_of_instances_required:\n","        diff_check = all_differences[i]\n","        mapping = diff_dict[diff_check]\n","        if mapping[0] not in already_mapped_keypoints and mapping[1] not in already_mapped_segmentation:\n","            final_mapping.append(mapping)\n","            already_mapped_keypoints.add(mapping[0])\n","            already_mapped_segmentation.add(mapping[1])\n","        i+=1\n","\n","    return final_mapping\n","\n","def overlay_mask_on_image(img, mask, alpha=0.5):\n","  colors = [[0,255,0], #'green'\n","            [204,0,102], #'magenta',\n","            [204, 102, 0], #'gold',\n","            [0,0,255], #'blue',\n","            [0,0,0], #'black',\n","            [255, 0, 0]] #'red'\n","  color = random.choice(colors)\n","  r = mask*color[0]\n","  g = mask*color[1]\n","  b = mask*color[2]\n","\n","  contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","  bgr_mask = np.asarray([b,g,r]).swapaxes(0,2).swapaxes(0,1)\n","  img = np.asarray(img, np.float32)\n","  masked_img = cv2.addWeighted(bgr_mask, alpha, img, 1-alpha, 0, img)\n","  masked_img = cv2.drawContours(masked_img, contours, -1, color, 2)\n","\n","  return masked_img\n"]},{"cell_type":"markdown","metadata":{"id":"NFe9tgAIRuH0"},"source":["### **Step 3** : Obtain your Outputs\n","\n","In this step, we run inference and obtain results by simply clicking the *Play* button next to the code cell below.\n","\n","As the code cell runs, you will see feedback printed out under the cell as the model runs on each subsequent image, and once complete it will give a confirmation that describes where you can view your outputs at.\n","\n","If you want to view the outputs in this notebook, set ``viz = True`` in the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ninsM758Yfcz","cellView":"form"},"outputs":[],"source":["#@title\n","\n","viz = False\n","\n","import traceback\n","import signal\n","import matplotlib.pyplot as plt\n","warnings.filterwarnings(\"ignore\")\n","\n","def handler(signum, frame):\n","  # print(\"Forever is over!\")\n","  raise Exception(\"Timed out\")\n","\n","## Create required output folders\n","ct = datetime.datetime.now()\n","date_str = ct.strftime(\"%Y_%d_%m_%H%M%S\")\n","output_dir = os.path.join(output_dir, 'output_'+ date_str)\n","os.makedirs(output_dir)\n","os.makedirs(os.path.join(output_dir, 'predicted_images'))\n","os.makedirs(os.path.join(output_dir, 'masks'))\n","print(\"* Begun prediction...\")\n","\n","final_intersection=[]\n","number_of_files=len([name for name in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, name))])\n","count=1\n","nseg = 20\n","\n","# if input_dir[-1] == '/':\n","#     input_dir_select_all = input_dir+'*'\n","# else:\n","#     input_dir_select_all=input_dir+'/*'\n","\n","input_dir_select_all = os.path.join(input_dir, '*')\n","\n","all_files = glob.glob(input_dir_select_all)\n","\n","final_intersection=[]\n","number_of_files=len([name for name in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, name))])\n","count=1\n","debug = True\n","# if input_dir[-1]=='/':\n","#     input_dir_select_all=input_dir+'*'\n","# else:\n","#     input_dir_select_all=input_dir+'/*'\n","\n","for filename in tqdm(glob.glob(os.path.join(input_dir, '*'))):\n","    # print (\"Processing image \",count,'/',number_of_files)\n","    count+=1\n","    # if count==10:\n","    #   break\n","    # if count<637:\n","    #   continue\n","    # print(filename)\n","    try:\n","      im = cv2.imread(filename)\n","\n","      outputs1 = predictor1(im)\n","      outputs2 = predictor2(im)\n","      im_temp=copy.deepcopy(im)\n","\n","      mapping_segmentation_to_keypoint = find_mappings(outputs1, outputs2)\n","      v = False\n","    except Exception:\n","        if debug:\n","          print(filename)\n","          traceback.print_exc()\n","      # print(len(mapping_segmentation_to_keypoint))\n","    for i in range(len(mapping_segmentation_to_keypoint)):\n","      im = copy.deepcopy(im_temp)\n","      signal.signal(signal.SIGALRM, handler)\n","      signal.alarm(60)\n","      try:\n","          instance_no = mapping_segmentation_to_keypoint[i][0] #Keypoint instance\n","          index = mapping_segmentation_to_keypoint[i][1] #Segmentation instance\n","          diff_arr=[]\n","          tens=outputs1[\"instances\"].pred_keypoints[instance_no]\n","          kp_df=tens.cpu().numpy()\n","\n","          midpoint = kp_df[:,:2].astype(int)\n","\n","          x,y=zip(*midpoint)\n","          lspace=np.linspace(min(x),max(x),100)\n","          z=np.polyfit(x,y,5)\n","          draw_x=lspace\n","          draw_y=np.polyval(z,draw_x)\n","          draw_points = (np.asarray([draw_x, draw_y]).T).astype(np.int32)\n","          im=cv2.polylines(im, [draw_points], False, (255,255,255),thickness=2)\n","          intersection_points = divide_curve_into_n_segments(nseg, z, x, y)\n","          slopes,slope_calc_x1,slope_calc_y1,slope_calc_x2,slope_calc_y2,xvalues,yvalues=calculateSlopes(im,intersection_points,z)\n","\n","          intersection=findIntersections(outputs2[\"instances\"].pred_masks[index],slope_calc_x1,slope_calc_y1,slope_calc_x2,slope_calc_y2,xvalues,yvalues)\n","          area_in_sq_px=sum(sum(outputs2[\"instances\"].pred_masks[index])).cpu().numpy()\n","          cv2.imwrite(os.path.join(output_dir,'masks/'+\".\".join((filename.split(\"/\")[-1]).split('.')[:-1])+'.Whale.'+str(instance_no+1)+'.jpg'),outputs2[\"instances\"].pred_masks[index].cpu().numpy().astype('float32')*255)\n","\n","          im = markWhaleID(im,instance_no,index,outputs2[\"instances\"])\n","          whaleLength = calculateWhaleLength(int(tens[0][0]),int(tens[0][1]),int(tens[4][0]),int(tens[4][1]))\n","          intersection_val = [\".\".join((filename.split(\"/\")[-1]).split('.')[:-1]),instance_no+1,int(tens[0][0]),int(tens[0][1]),int(tens[4][0]),int(tens[4][1]),whaleLength,area_in_sq_px]\n","          for key,val in intersection.items():\n","              if len(val)==2:\n","                  im = cv2.line(im, tuple(val[0]), tuple(val[1]), thickness=2, color=(0, 255, 0))\n","                  if (np.polyval(z,val[0][0]) - val[0][1])>0: ## So that all (X1,Y1) are on one side and (X2,Y2) are on the other side\n","                    intersection_val.append(val[0][0])\n","                    intersection_val.append(val[0][1])\n","                    intersection_val.append(val[1][0])\n","                    intersection_val.append(val[1][1])\n","                  else:\n","                    intersection_val.append(val[1][0])\n","                    intersection_val.append(val[1][1])\n","                    intersection_val.append(val[0][0])\n","                    intersection_val.append(val[0][1])\n","              else:\n","                  intersection_val.append(-1)\n","                  intersection_val.append(-1)\n","                  intersection_val.append(-1)\n","                  intersection_val.append(-1)\n","          final_intersection.append(intersection_val)\n","\n","          v = Visualizer(im[:, :, ::-1],\n","                  # scale=0.8\n","                  scale=1\n","                  )\n","          try:\n","            if v:\n","\n","              # print(output_dir+'predicted_images/'+\".\".join((filename.split(\"/\")[-1]).split('.')[:-1])+'.WidthMarked.jpg')\n","              pred_mask = outputs2[\"instances\"].pred_masks[index].cpu().numpy().astype('float32')\n","              img_with_overlayed_mask = overlay_mask_on_image(im, pred_mask, alpha=0.2)\n","              cv2.imwrite(os.path.join(output_dir,'predicted_images/'+\".\".join((filename.split(\"/\")[-1]).split('.')[:-1])+f'Whale.{str(instance_no+1)}.WidthMarked.jpg'),img_with_overlayed_mask)\n","              if viz:\n","                out1 = v.draw_instance_predictions(outputs2[\"instances\"].to(\"cpu\"))\n","                plt.figure(figsize=(12,8))\n","                plt.imshow(cv2.cvtColor(out1.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n","                plt.show()\n","          # except:\n","          except Exception as e:\n","              if debug:\n","                print(repr(e))\n","                traceback.print_exc()\n","              # print(\"Error while processing \",filename.split(\"/\")[-1])\n","              # print(sys.exc_info()[0])\n","              # #raise\n","      except Exception:\n","        if debug:\n","          print(filename)\n","          traceback.print_exc()\n","    # try:\n","    #   if v:\n","    #     out1 = v.draw_instance_predictions(outputs2[\"instances\"].to(\"cpu\"))\n","    #     # print(output_dir+'predicted_images/'+\".\".join((filename.split(\"/\")[-1]).split('.')[:-1])+'.WidthMarked.jpg')\n","    #     cv2.imwrite(os.path.join(output_dir,'predicted_images/'+\".\".join((filename.split(\"/\")[-1]).split('.')[:-1])+f'WidthMarked.jpg'),out1.get_image()[:, :, ::-1])\n","    #     # plt.figure(figsize=(12,8))\n","    #     # plt.imshow(cv2.cvtColor(out1.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n","    #     # plt.show()\n","    # # except:\n","    # except Exception:\n","    #     if debug:\n","    #       print(filename)\n","    #       traceback.print_exc()\n","    #     # print(\"Error while processing \",filename.split(\"/\")[-1])\n","    #     # print(sys.exc_info()[0])\n","    #     # #raise\n","\n","\n","# cols=[\"Image.ID\",\"Whale.ID\",\"Rostrum.X\",\"Rostrum.Y\",\"Fluke.Middle.X\",\"Fluke.Middle.Y\",\"Total.Length\",\"Area\",'Endpoint.Width.5.X1',\n","#       'Endpoint.Width.5.Y1', 'Endpoint.Width.5.X2', 'Endpoint.Width.5.Y2', 'Endpoint.Width.10.X1','Endpoint.Width.10.Y1',\n","#       'Endpoint.Width.10.X2', 'Endpoint.Width.10.Y2', 'Endpoint.Width.15.X1','Endpoint.Width.15.Y1', 'Endpoint.Width.15.X2',\n","#       'Endpoint.Width.15.Y2', 'Endpoint.Width.20.X1','Endpoint.Width.20.Y1', 'Endpoint.Width.20.X2', 'Endpoint.Width.20.Y2',\n","#       'Endpoint.Width.25.X1','Endpoint.Width.25.Y1', 'Endpoint.Width.25.X2', 'Endpoint.Width.25.Y2', 'Endpoint.Width.30.X1',\n","#       'Endpoint.Width.30.Y1', 'Endpoint.Width.30.X2', 'Endpoint.Width.30.Y2', 'Endpoint.Width.35.X1','Endpoint.Width.35.Y1',\n","#       'Endpoint.Width.35.X2', 'Endpoint.Width.35.Y2', 'Endpoint.Width.40.X1','Endpoint.Width.40.Y1', 'Endpoint.Width.40.X2',\n","#       'Endpoint.Width.40.Y2', 'Endpoint.Width.45.X1','Endpoint.Width.45.Y1', 'Endpoint.Width.45.X2', 'Endpoint.Width.45.Y2',\n","#       'Endpoint.Width.50.X1','Endpoint.Width.50.Y1', 'Endpoint.Width.50.X2', 'Endpoint.Width.50.Y2', 'Endpoint.Width.55.X1',\n","#       'Endpoint.Width.55.Y1', 'Endpoint.Width.55.X2', 'Endpoint.Width.55.Y2', 'Endpoint.Width.60.X1','Endpoint.Width.60.Y1',\n","#       'Endpoint.Width.60.X2', 'Endpoint.Width.60.Y2', 'Endpoint.Width.65.X1','Endpoint.Width.65.Y1', 'Endpoint.Width.65.X2',\n","#       'Endpoint.Width.65.Y2', 'Endpoint.Width.70.X1','Endpoint.Width.70.Y1', 'Endpoint.Width.70.X2', 'Endpoint.Width.70.Y2',\n","#       'Endpoint.Width.75.X1','Endpoint.Width.75.Y1', 'Endpoint.Width.75.X2', 'Endpoint.Width.75.Y2', 'Endpoint.Width.80.X1',\n","#       'Endpoint.Width.80.Y1', 'Endpoint.Width.80.X2', 'Endpoint.Width.80.Y2', 'Endpoint.Width.85.X1','Endpoint.Width.85.Y1',\n","#       'Endpoint.Width.85.X2', 'Endpoint.Width.85.Y2', 'Endpoint.Width.90.X1','Endpoint.Width.90.Y1', 'Endpoint.Width.90.X2',\n","#       'Endpoint.Width.90.Y2', 'Endpoint.Width.95.X1','Endpoint.Width.95.Y1', 'Endpoint.Width.95.X2', 'Endpoint.Width.95.Y2']\n","\n","# intersection_df=pd.DataFrame(final_intersection,columns=cols)\n","\n","# j=0\n","# for i in range(8,intersection_df.shape[1],4):\n","#     j+=5\n","#     cols=intersection_df.columns[i:i+4]\n","#     intersection_df[\"width_\"+str(j)]=np.sqrt((intersection_df[cols[0]]-intersection_df[cols[2]])**2+(intersection_df[cols[1]]-intersection_df[cols[3]])**2)\n","# intersection_df.replace(-1,np.nan,inplace=True)\n","\n","\n","\n","# intersection_df.to_csv(os.path.join(output_dir,\"predicted_data.csv\"))\n","\n","# print(\"All images have been processed. Outputs can be found in \",output_dir)\n","\n","\n","# except:\n","#     print(\"Error while processing \", filename.split(\"/\")[-1])\n","#     print(sys.exc_info()[0])\n","#     #raise\n","\n","\n","cols = [\"Image.ID\", \"Whale.ID\", \"Rostrum.X\", \"Rostrum.Y\", \"Fluke.Middle.X\", \"Fluke.Middle.Y\", \"Total.Length\", \"Area\",\n","      'Endpoint.Width.5.X1', 'Endpoint.Width.5.Y1', 'Endpoint.Width.5.X2', 'Endpoint.Width.5.Y2',\n","      'Endpoint.Width.10.X1', 'Endpoint.Width.10.Y1', 'Endpoint.Width.10.X2', 'Endpoint.Width.10.Y2',\n","      'Endpoint.Width.15.X1', 'Endpoint.Width.15.Y1', 'Endpoint.Width.15.X2', 'Endpoint.Width.15.Y2',\n","      'Endpoint.Width.20.X1', 'Endpoint.Width.20.Y1', 'Endpoint.Width.20.X2', 'Endpoint.Width.20.Y2',\n","      'Endpoint.Width.25.X1', 'Endpoint.Width.25.Y1', 'Endpoint.Width.25.X2', 'Endpoint.Width.25.Y2',\n","      'Endpoint.Width.30.X1', 'Endpoint.Width.30.Y1', 'Endpoint.Width.30.X2', 'Endpoint.Width.30.Y2',\n","      'Endpoint.Width.35.X1', 'Endpoint.Width.35.Y1', 'Endpoint.Width.35.X2', 'Endpoint.Width.35.Y2',\n","      'Endpoint.Width.40.X1', 'Endpoint.Width.40.Y1', 'Endpoint.Width.40.X2', 'Endpoint.Width.40.Y2',\n","      'Endpoint.Width.45.X1', 'Endpoint.Width.45.Y1', 'Endpoint.Width.45.X2', 'Endpoint.Width.45.Y2',\n","      'Endpoint.Width.50.X1', 'Endpoint.Width.50.Y1', 'Endpoint.Width.50.X2', 'Endpoint.Width.50.Y2',\n","      'Endpoint.Width.55.X1', 'Endpoint.Width.55.Y1', 'Endpoint.Width.55.X2', 'Endpoint.Width.55.Y2',\n","      'Endpoint.Width.60.X1', 'Endpoint.Width.60.Y1', 'Endpoint.Width.60.X2', 'Endpoint.Width.60.Y2',\n","      'Endpoint.Width.65.X1', 'Endpoint.Width.65.Y1', 'Endpoint.Width.65.X2', 'Endpoint.Width.65.Y2',\n","      'Endpoint.Width.70.X1', 'Endpoint.Width.70.Y1', 'Endpoint.Width.70.X2', 'Endpoint.Width.70.Y2',\n","      'Endpoint.Width.75.X1', 'Endpoint.Width.75.Y1', 'Endpoint.Width.75.X2', 'Endpoint.Width.75.Y2',\n","      'Endpoint.Width.80.X1', 'Endpoint.Width.80.Y1', 'Endpoint.Width.80.X2', 'Endpoint.Width.80.Y2',\n","      'Endpoint.Width.85.X1', 'Endpoint.Width.85.Y1', 'Endpoint.Width.85.X2', 'Endpoint.Width.85.Y2',\n","      'Endpoint.Width.90.X1', 'Endpoint.Width.90.Y1', 'Endpoint.Width.90.X2', 'Endpoint.Width.90.Y2',\n","      'Endpoint.Width.95.X1', 'Endpoint.Width.95.Y1', 'Endpoint.Width.95.X2', 'Endpoint.Width.95.Y2']\n","\n","intersection_df = pd.DataFrame(final_intersection, columns=cols)\n","\n","j = 0\n","for i in range(8, intersection_df.shape[1], 4):\n","    j += 5\n","    cols = intersection_df.columns[i:i+4]\n","    intersection_df[\"width_\"+str(j)] = np.sqrt((intersection_df[cols[0]]-intersection_df[cols[2]])**2+(intersection_df[cols[1]]-intersection_df[cols[3]])**2)\n","\n","intersection_df.replace(-1, np.nan, inplace=True)\n","\n","nan_array = np.empty(intersection_df.shape[0])\n","nan_array[:] = np.nan\n","\n","intersection_df[\"Peduncle.X\"] = nan_array\n","intersection_df[\"Peduncle.Y\"] = nan_array\n","intersection_df[\"Dorsal.Fin.Start.X\"] = nan_array\n","intersection_df[\"Dorsal.Fin.Start.Y\"] = nan_array\n","intersection_df[\"Dorsal.Fin.End.X\"] = nan_array\n","intersection_df[\"Dorsal.Fin.End.Y\"] = nan_array\n","intersection_df[\"Fluke.Endpoint.X1\"] = nan_array\n","intersection_df[\"Fluke.Endpoint.Y1\"] = nan_array\n","intersection_df[\"Fluke.Endpoint.X2\"] = nan_array\n","intersection_df[\"Fluke.Endpoint.Y2\"] = nan_array\n","intersection_df[\"Blowhole.X\"] = nan_array\n","intersection_df[\"Blowole.Y\"] = nan_array\n","intersection_df[\"Eye.X1\"] = nan_array\n","intersection_df[\"Eye.Y1\"] = nan_array\n","intersection_df[\"Eye.X2\"] = nan_array\n","intersection_df[\"Eye.Y2\"] = nan_array\n","\n","intersection_df.to_csv(os.path.join(output_dir, \"predicted_data.csv\")) # Write all data to predicted_data.csv\n","\n","print(\"* All images have been processed. Outputs can be found in \", output_dir)"]},{"cell_type":"markdown","metadata":{"id":"i_UqPWsCWezb"},"source":["## **Completion**\n","\n","\n","---\n","\n","\n","\n","To view your outputs, navigate to your output directory in your Google Drive and look for the subfolder name that the code cell above printed out. The subfolder has the naming convention of:  `output_<date>_<time> `\n","This subfolder will contain a folder of predicted images, a folder of masks, and a CSV file quantifying the predictions.\n","\n","If your runtime has not timed out you can re-run the module on a different batch of images starting from the beginning of the Running Inference section (and can skip the sections above it as long as the green check marks next to the cells above remains and your base folder has not changed). You can either remove the current batch of images from your `input_dir` and upload the new batch and not change the location. Or simply give a different location path to `input_dir` referring to a new folder with the new images."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}